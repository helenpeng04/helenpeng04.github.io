import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import spacy
from sklearn.feature_extraction.text import CountVectorizer
from hdbscan import HDBSCAN
import plotly.io as pio
# Load the data
file_path = r"C:/Users/helen/Github/RR_2024/Helen/bugs/cleaned_verbal_resp_onlybugs.csv"
df = pd.read_excel(file_path)
file_path = r"C:/Users/helen/Github/RR_2024/Helen/bugs/cleaned_verbal_resp_onlybugs.csv"
df = pd.read_excel(file_path)
# Load the pre-trained SBERT model
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
# Convert responses into a list of strings (documents)
docs = df["Response"].dropna().astype(str).tolist()
# Generate embeddings for each document
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Optionally store embeddings in dataframe:
# df['Embeddings'] = embeddings.tolist()
nlp = spacy.load("en_core_web_sm")  # Load small English spaCy model
# Create a CountVectorizer with custom stop words from spaCy
vectorizer_model = CountVectorizer(
ngram_range=(1, 2),
stop_words=list(nlp.Defaults.stop_words)
)
# HDBSCAN model to discover clusters (topics)
cluster_model = HDBSCAN(min_cluster_size=20,
min_samples=10,
metric='euclidean',
prediction_data = True)
# Create BERTopic model
topic_model = BERTopic(
embedding_model=sentence_model,
language='English',
verbose=True,
calculate_probabilities=False,
vectorizer_model=vectorizer_model,
n_gram_range=(1,2),
min_topic_size=100,
hdbscan_model=cluster_model
)
# Fit BERTopic model to our documents and embeddings
topic, probs = topic_model.fit_transform(docs, embeddings)
# Get frequency table of topics (top 5)
freq = topic_model.get_topic_freq()
print(freq[:10])
print('Num of topics:', len(freq))
# Get keywords for the first two topics
print(topic_model.get_topic(0))  # Topic 0 keywords
print(topic_model.get_topic(1))  # Topic 1 keywords
# Get representative documents for first two topics
print(topic_model.get_representative_docs(0))
print(topic_model.get_representative_docs(1))
# Generate plots
fig = topic_model.visualize_barchart()
fig2 = topic_model.visualize_heatmap()
fig3 = topic_model.visualize_hierarchy()
# Save figures as HTML files
fig.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\barchart.html")
fig2.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\heatmap.html")
fig3.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\hierarchy.html")
cluster_model = HDBSCAN(min_cluster_size=20,
getwd()
min_samples=5,
import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import spacy
from sklearn.feature_extraction.text import CountVectorizer
from hdbscan import HDBSCAN
import plotly.io as pio
# Load the data
file_path = r"C:/Users/helen/Github/RR_2024/Helen/bugs/cleaned_verbal_resp_onlybugs.csv"
df = pd.read_excel(file_path)
# Load the pre-trained SBERT model
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
# Convert responses into a list of strings (documents)
docs = df["Response"].dropna().astype(str).tolist()
# Generate embeddings for each document
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Optionally store embeddings in dataframe:
# df['Embeddings'] = embeddings.tolist()
nlp = spacy.load("en_core_web_sm")  # Load small English spaCy model
# Create a CountVectorizer with custom stop words from spaCy
vectorizer_model = CountVectorizer(
ngram_range=(1, 2),
stop_words=list(nlp.Defaults.stop_words)
)
# HDBSCAN model to discover clusters (topics)
cluster_model = HDBSCAN(min_cluster_size=20,
min_samples=5,
metric='euclidean',
prediction_data = True)
# Create BERTopic model
topic_model = BERTopic(
embedding_model=sentence_model,
language='English',
verbose=True,
calculate_probabilities=False,
vectorizer_model=vectorizer_model,
n_gram_range=(1,2),
min_topic_size=100,
hdbscan_model=cluster_model
)
# Fit BERTopic model to our documents and embeddings
topic, probs = topic_model.fit_transform(docs, embeddings)
# Get frequency table of topics (top 5)
freq = topic_model.get_topic_freq()
print(freq[:10])
print('Num of topics:', len(freq))
# Get keywords for the first two topics
print(topic_model.get_topic(0))  # Topic 0 keywords
print(topic_model.get_topic(1))  # Topic 1 keywords
# Get representative documents for first two topics
print(topic_model.get_representative_docs(0))
print(topic_model.get_representative_docs(1))
# Generate plots
fig = topic_model.visualize_barchart()
fig2 = topic_model.visualize_heatmap()
fig3 = topic_model.visualize_hierarchy()
# Save figures as HTML files
fig.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\barchart.html")
fig2.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\heatmap.html")
fig3.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\hierarchy.html")
