import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import spacy
from sklearn.feature_extraction.text import CountVectorizer
from hdbscan import HDBSCAN
import plotly.io as pio
# Load the data
file_path = r"C:/Users/helen/Github/RR_2024/Helen/bugs/cleaned_verbal_resp_onlybugs.csv"
df = pd.read_excel(file_path)
file_path = r"C:/Users/helen/Github/RR_2024/Helen/bugs/cleaned_verbal_resp_onlybugs.csv"
df = pd.read_excel(file_path)
# Load the pre-trained SBERT model
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
# Convert responses into a list of strings (documents)
docs = df["Response"].dropna().astype(str).tolist()
# Generate embeddings for each document
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Optionally store embeddings in dataframe:
# df['Embeddings'] = embeddings.tolist()
nlp = spacy.load("en_core_web_sm")  # Load small English spaCy model
# Create a CountVectorizer with custom stop words from spaCy
vectorizer_model = CountVectorizer(
ngram_range=(1, 2),
stop_words=list(nlp.Defaults.stop_words)
)
# HDBSCAN model to discover clusters (topics)
cluster_model = HDBSCAN(min_cluster_size=20,
min_samples=10,
metric='euclidean',
prediction_data = True)
# Create BERTopic model
topic_model = BERTopic(
embedding_model=sentence_model,
language='English',
verbose=True,
calculate_probabilities=False,
vectorizer_model=vectorizer_model,
n_gram_range=(1,2),
min_topic_size=100,
hdbscan_model=cluster_model
)
# Fit BERTopic model to our documents and embeddings
topic, probs = topic_model.fit_transform(docs, embeddings)
# Get frequency table of topics (top 5)
freq = topic_model.get_topic_freq()
print(freq[:10])
print('Num of topics:', len(freq))
# Get keywords for the first two topics
print(topic_model.get_topic(0))  # Topic 0 keywords
print(topic_model.get_topic(1))  # Topic 1 keywords
# Get representative documents for first two topics
print(topic_model.get_representative_docs(0))
print(topic_model.get_representative_docs(1))
# Generate plots
fig = topic_model.visualize_barchart()
fig2 = topic_model.visualize_heatmap()
fig3 = topic_model.visualize_hierarchy()
# Save figures as HTML files
fig.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\barchart.html")
fig2.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\heatmap.html")
fig3.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\hierarchy.html")
cluster_model = HDBSCAN(min_cluster_size=20,
getwd()
min_samples=5,
import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import spacy
from sklearn.feature_extraction.text import CountVectorizer
from hdbscan import HDBSCAN
import plotly.io as pio
# Load the data
file_path = r"C:/Users/helen/Github/RR_2024/Helen/bugs/cleaned_verbal_resp_onlybugs.csv"
df = pd.read_excel(file_path)
# Load the pre-trained SBERT model
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
# Convert responses into a list of strings (documents)
docs = df["Response"].dropna().astype(str).tolist()
# Generate embeddings for each document
embeddings = sentence_model.encode(docs, show_progress_bar=True)
# Optionally store embeddings in dataframe:
# df['Embeddings'] = embeddings.tolist()
nlp = spacy.load("en_core_web_sm")  # Load small English spaCy model
# Create a CountVectorizer with custom stop words from spaCy
vectorizer_model = CountVectorizer(
ngram_range=(1, 2),
stop_words=list(nlp.Defaults.stop_words)
)
# HDBSCAN model to discover clusters (topics)
cluster_model = HDBSCAN(min_cluster_size=20,
min_samples=5,
metric='euclidean',
prediction_data = True)
# Create BERTopic model
topic_model = BERTopic(
embedding_model=sentence_model,
language='English',
verbose=True,
calculate_probabilities=False,
vectorizer_model=vectorizer_model,
n_gram_range=(1,2),
min_topic_size=100,
hdbscan_model=cluster_model
)
# Fit BERTopic model to our documents and embeddings
topic, probs = topic_model.fit_transform(docs, embeddings)
# Get frequency table of topics (top 5)
freq = topic_model.get_topic_freq()
print(freq[:10])
print('Num of topics:', len(freq))
# Get keywords for the first two topics
print(topic_model.get_topic(0))  # Topic 0 keywords
print(topic_model.get_topic(1))  # Topic 1 keywords
# Get representative documents for first two topics
print(topic_model.get_representative_docs(0))
print(topic_model.get_representative_docs(1))
# Generate plots
fig = topic_model.visualize_barchart()
fig2 = topic_model.visualize_heatmap()
fig3 = topic_model.visualize_hierarchy()
# Save figures as HTML files
fig.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\barchart.html")
fig2.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\heatmap.html")
fig3.write_html(r"C:\Users\helen\Github\RR_2024\Helen\bugs\hierarchy.html")
library(tidyverse)
student_data <- read_csv("https://raw.githubusercontent.com/FSKoerner/F25-36315-data/refs/heads/main/students.csv")
student_data_quant <- student_data |> select(RaisedHands, VisitedResources, AnnouncementsView, Discussion)
student_data_scaled <- scale(student_data_quant)
student_dist <- dist(student_data_scaled, method = "euclidean")
dim(as.matrix(student_dist))
mean(as.matrix(student_dist)[1,])
mean(as.matrix(student_dist)[,1])
student_hc <- hcluster(student_dist, method = "complete")
student_hc <- hclust(student_dist, method = "complete")
student_hc_dend <- as.dendrogram(student_hc)
plot(student_hc_dend)
student_hc_dend <- as.dendrogram(student_hc)
plot(student_hc_dend)
# Install the package if you do not have it - but remember to comment out this
# line before knitting!
# install.packages("dendextend")
library(dendextend)
#what are the k = 3 clusters identified by the dendrogram?
student_hc_dend <- set(student_hc_dend, "branches_k_color", k = 3)
plot(student_hc_dend)
grade_colors = ifelse(student_data$Grade == "H", "blue",
ifelse(student_data$Grade == "M", "green", "red"))
student_hc_dend <- set(student_hc_dend, "labels_colors",
order_value = TRUE, grade_colors)
plot(student_hc_dend)
student_data_quant <- student_data |> select(RaisedHands, VisitedResources, AnnouncementsView, Discussion)
student_pca <- prcomp(student_data_quant, center = TRUE, scale. = TRUE)
summary(student_pca)
student_pca |> fviz_eig(addlabels = TRUE, ylim = c(0, 60))
# Comment out the installation line if/after you use it!
# And make sure to restart R after installing the package...
# install.packages("factoextra")
library(factoextra)
student_pca |> fviz_eig(addlabels = TRUE, ylim = c(0, 60))
student_pca |> fviz_eig(addlabels = TRUE)
student_pca |> fviz_eig(addlabels = TRUE) +
geom_hline(yintercept = 100 * (1 / ncol(student_data_quant)))
student_pc_matrix <- student_pca$x
student_data |> ggplot(aes(x = pc1, y = pc2)) +
geom_point(alpha = 0.5, aes(color = Grade)) +
labs(x = "PC 1", y = "PC 2")
student_data  <- student_data  |>
mutate(pc1 = student_pc_matrix[,1],
pc2 = student_pc_matrix[,2])
student_data |> ggplot(aes(x = pc1, y = pc2)) +
geom_point(alpha = 0.5, aes(color = Grade)) +
labs(x = "PC 1", y = "PC 2")
student_data |> ggplot(aes(x = pc1, y = pc2)) +
geom_point(alpha = 0.5, aes(color = Grade)) +
labs(x = "PC 1", y = "PC 2") +
theme_bw()
student_data |> ggplot(aes(x = pc1, y = pc2)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", se = FALSE) +
labs(x = "PC 1", y = "PC 2") +
theme_bw()
